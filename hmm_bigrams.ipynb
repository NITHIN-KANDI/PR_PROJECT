{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNY3PMfCUzXksPih3p09Ph5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NITHIN-KANDI/PR_PROJECT/blob/main/hmm_bigrams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VealP0lMJIhB",
        "outputId": "d2189373-8a52-4be9-add3-1efd410cb56e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk import ngrams\n",
        "import nltk\n",
        "\n",
        "# Ensure NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define the HMM model components with Laplace smoothing and empty-sequence handling\n",
        "class HMM:\n",
        "    def __init__(self, n_states, n_observations):\n",
        "        self.n_states = n_states  # Number of hidden states\n",
        "        self.n_observations = n_observations  # Number of possible observations\n",
        "\n",
        "        # Initialize the transition probabilities (states to states)\n",
        "        self.transition_probs = np.full((n_states, n_states), 1.0 / n_states)\n",
        "\n",
        "        # Initialize the emission probabilities (states to observations)\n",
        "        self.emission_probs = np.full((n_states, n_observations), 1.0 / n_observations)\n",
        "\n",
        "        # Initialize start probabilities\n",
        "        self.start_probs = np.full(n_states, 1.0 / n_states)\n",
        "\n",
        "    def train(self, sequences, state_sequences, alpha=1.0):\n",
        "        # Calculate start probabilities with smoothing\n",
        "        for state_seq in state_sequences:\n",
        "            self.start_probs[state_seq[0]] += 1\n",
        "        self.start_probs += alpha  # Laplace smoothing\n",
        "        self.start_probs /= self.start_probs.sum()\n",
        "\n",
        "        # Calculate transition probabilities with smoothing\n",
        "        for state_seq in state_sequences:\n",
        "            for i in range(len(state_seq) - 1):\n",
        "                self.transition_probs[state_seq[i], state_seq[i + 1]] += 1\n",
        "        self.transition_probs += alpha  # Laplace smoothing\n",
        "        self.transition_probs /= self.transition_probs.sum(axis=1, keepdims=True)\n",
        "\n",
        "        # Calculate emission probabilities with smoothing\n",
        "        for seq, state_seq in zip(sequences, state_sequences):\n",
        "            for obs, state in zip(seq, state_seq):\n",
        "                self.emission_probs[state, obs] += 1\n",
        "        self.emission_probs += alpha  # Laplace smoothing\n",
        "        self.emission_probs /= self.emission_probs.sum(axis=1, keepdims=True)\n",
        "\n",
        "    def viterbi(self, sequence):\n",
        "        # Check if the sequence is empty\n",
        "        if len(sequence) == 0:\n",
        "            return [0]  # Return a default state or handle appropriately\n",
        "\n",
        "        # Initialize matrices\n",
        "        T = len(sequence)\n",
        "        viterbi_probs = np.zeros((self.n_states, T))\n",
        "        backpointer = np.zeros((self.n_states, T), dtype=int)\n",
        "\n",
        "        # Initialize the first column\n",
        "        for s in range(self.n_states):\n",
        "            viterbi_probs[s, 0] = self.start_probs[s] * self.emission_probs[s, sequence[0]]\n",
        "            backpointer[s, 0] = 0\n",
        "\n",
        "        # Fill the Viterbi matrix\n",
        "        for t in range(1, T):\n",
        "            for s in range(self.n_states):\n",
        "                probabilities = viterbi_probs[:, t - 1] * self.transition_probs[:, s] * self.emission_probs[s, sequence[t]]\n",
        "                viterbi_probs[s, t] = np.max(probabilities)\n",
        "                backpointer[s, t] = np.argmax(probabilities)\n",
        "\n",
        "        # Backtrace to get the most probable state sequence\n",
        "        best_path = np.zeros(T, dtype=int)\n",
        "        best_path[-1] = np.argmax(viterbi_probs[:, T - 1])\n",
        "        for t in range(T - 2, -1, -1):\n",
        "            best_path[t] = backpointer[best_path[t + 1], t + 1]\n",
        "\n",
        "        return best_path\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load filtered datasets\n",
        "train_data = pd.read_csv('/content/filtered_train_data.csv')\n",
        "test_data = pd.read_csv('/content/filtered_test_data.csv')\n",
        "\n",
        "# Define state mapping and vocabulary\n",
        "state_mapping = {'truth': 0, 'partial truth': 1, 'partial lie': 2, 'lie': 3}\n",
        "n_states = len(state_mapping)\n",
        "\n",
        "# Initialize stopwords and stemmer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text_with_bigrams(text):\n",
        "    # Tokenize, remove stopwords, and stem\n",
        "    tokens = [stemmer.stem(word) for word in word_tokenize(text.lower()) if word.isalnum() and word not in stop_words]\n",
        "    bigrams = [' '.join(gram) for gram in ngrams(tokens, 2)]  # Generate bigrams\n",
        "    return bigrams\n",
        "\n",
        "# Apply preprocessing and create vocabulary\n",
        "train_data['tokens'] = train_data['statement'].apply(preprocess_text_with_bigrams)\n",
        "all_bigrams = [bigram for bigrams in train_data['tokens'] for bigram in bigrams]\n",
        "vocab = list(set(all_bigrams))\n",
        "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "n_observations = len(vocab)\n",
        "\n",
        "# Encode tokens and labels\n",
        "def encode_tokens(tokens):\n",
        "    return [word_to_index.get(token, -1) for token in tokens if token in word_to_index]\n",
        "\n",
        "encoded_train_sequences = [encode_tokens(tokens) for tokens in train_data['tokens']]\n",
        "encoded_state_sequences = [[state_mapping[label]] for label in train_data['label']]\n"
      ],
      "metadata": {
        "id": "03SE0CsQKxL-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the HMM with smoothing\n",
        "hmm_model = HMM(n_states=n_states, n_observations=n_observations)\n",
        "hmm_model.train(encoded_train_sequences, encoded_state_sequences, alpha=1.0)  # Alpha can be adjusted for smoothing\n",
        "print(\"HMM model training completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AVkgfP4K5Nr",
        "outputId": "5688f6ab-4836-4a7a-cf2a-92685386c527"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HMM model training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_sentence(sentence, hmm_model, word_to_index, state_mapping):\n",
        "    # Tokenize and encode the sentence using bigrams\n",
        "    tokens = preprocess_text_with_bigrams(sentence)\n",
        "    encoded_tokens = [word_to_index.get(token, -1) for token in tokens if token in word_to_index]\n",
        "    if not encoded_tokens:\n",
        "        return \"Unknown\"  # If no tokens are recognized\n",
        "\n",
        "    # Use the Viterbi algorithm to find the best state path\n",
        "    best_path = hmm_model.viterbi(encoded_tokens)\n",
        "    return list(state_mapping.keys())[best_path[0]]\n",
        "\n",
        "# Test with a sample sentence\n",
        "sample_sentence = \"This is an entirely true statement.\"\n",
        "classification = classify_sentence(sample_sentence, hmm_model, word_to_index, state_mapping)\n",
        "print(f\"The sentence classification is: {classification}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP3wuVALK8Hz",
        "outputId": "f08d8f08-a91b-4474-9669-57ec6fec6002"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentence classification is: Unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the test set with bigrams\n",
        "test_data['tokens'] = test_data['statement'].apply(preprocess_text_with_bigrams)\n",
        "encoded_test_sequences = [encode_tokens(tokens) for tokens in test_data['tokens']]\n",
        "true_labels = [state_mapping[label] for label in test_data['label']]\n",
        "\n",
        "# Predict labels using the model\n",
        "predicted_labels = [hmm_model.viterbi(seq)[0] for seq in encoded_test_sequences]\n",
        "\n",
        "# Evaluation metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKhVMhQILB0S",
        "outputId": "5ed36ea6-0b60-4479-fb65-145c851b2573"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.35\n",
            "Precision: 0.30\n",
            "Recall: 0.35\n",
            "F1 Score: 0.30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}