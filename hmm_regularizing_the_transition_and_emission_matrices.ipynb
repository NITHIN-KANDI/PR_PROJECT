{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOw7SJ3ePSium79v/qqgaAo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NITHIN-KANDI/PR_PROJECT/blob/main/hmm_regularizing_the_transition_and_emission_matrices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCaophEeNr3M",
        "outputId": "81d1324c-c065-4c06-f1d9-33a096d1fc2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk import ngrams\n",
        "import nltk\n",
        "\n",
        "# Ensure NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class RegularizedHMM:\n",
        "    def __init__(self, n_states, n_observations, regularization=1e-3):\n",
        "        self.n_states = n_states  # Number of hidden states\n",
        "        self.n_observations = n_observations  # Number of possible observations\n",
        "        self.regularization = regularization  # Regularization term\n",
        "\n",
        "        # Initialize transition probabilities with regularization\n",
        "        self.transition_probs = np.full((n_states, n_states), 1.0 / n_states) + regularization\n",
        "\n",
        "        # Initialize emission probabilities with regularization\n",
        "        self.emission_probs = np.full((n_states, n_observations), 1.0 / n_observations) + regularization\n",
        "\n",
        "        # Initialize start probabilities with regularization\n",
        "        self.start_probs = np.full(n_states, 1.0 / n_states) + regularization\n",
        "\n",
        "    def train(self, sequences, state_sequences):\n",
        "        # Update start probabilities with regularization\n",
        "        for state_seq in state_sequences:\n",
        "            self.start_probs[state_seq[0]] += 1\n",
        "        self.start_probs = (self.start_probs + self.regularization) / self.start_probs.sum()\n",
        "\n",
        "        # Update transition probabilities with regularization\n",
        "        for state_seq in state_sequences:\n",
        "            for i in range(len(state_seq) - 1):\n",
        "                self.transition_probs[state_seq[i], state_seq[i + 1]] += 1\n",
        "        self.transition_probs = (self.transition_probs + self.regularization)\n",
        "        self.transition_probs /= self.transition_probs.sum(axis=1, keepdims=True)\n",
        "\n",
        "        # Update emission probabilities with regularization\n",
        "        for seq, state_seq in zip(sequences, state_sequences):\n",
        "            for obs, state in zip(seq, state_seq):\n",
        "                self.emission_probs[state, obs] += 1\n",
        "        self.emission_probs = (self.emission_probs + self.regularization)\n",
        "        self.emission_probs /= self.emission_probs.sum(axis=1, keepdims=True)\n",
        "\n",
        "    def viterbi(self, sequence):\n",
        "        T = len(sequence)\n",
        "        if T == 0:\n",
        "            return [0]  # Return default state if sequence is empty\n",
        "\n",
        "        # Initialize Viterbi matrices\n",
        "        viterbi_probs = np.zeros((self.n_states, T))\n",
        "        backpointer = np.zeros((self.n_states, T), dtype=int)\n",
        "\n",
        "        # Initialize the first column\n",
        "        for s in range(self.n_states):\n",
        "            viterbi_probs[s, 0] = self.start_probs[s] * self.emission_probs[s, sequence[0]]\n",
        "            backpointer[s, 0] = 0\n",
        "\n",
        "        # Fill in the Viterbi matrix\n",
        "        for t in range(1, T):\n",
        "            for s in range(self.n_states):\n",
        "                probabilities = viterbi_probs[:, t - 1] * self.transition_probs[:, s] * self.emission_probs[s, sequence[t]]\n",
        "                viterbi_probs[s, t] = np.max(probabilities)\n",
        "                backpointer[s, t] = np.argmax(probabilities)\n",
        "\n",
        "        # Backtrace to get the most probable state sequence\n",
        "        best_path = np.zeros(T, dtype=int)\n",
        "        best_path[-1] = np.argmax(viterbi_probs[:, T - 1])\n",
        "        for t in range(T - 2, -1, -1):\n",
        "            best_path[t] = backpointer[best_path[t + 1], t + 1]\n",
        "\n",
        "        return best_path\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load filtered datasets\n",
        "train_data = pd.read_csv('/content/filtered_train_data.csv')\n",
        "test_data = pd.read_csv('/content/filtered_test_data.csv')\n",
        "\n",
        "# Define state mapping and vocabulary\n",
        "state_mapping = {'truth': 0, 'partial truth': 1, 'partial lie': 2, 'lie': 3}\n",
        "n_states = len(state_mapping)\n",
        "\n",
        "# Initialize stopwords and stemmer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text_with_bigrams(text):\n",
        "    # Tokenize, remove stopwords, and stem\n",
        "    tokens = [stemmer.stem(word) for word in word_tokenize(text.lower()) if word.isalnum() and word not in stop_words]\n",
        "    bigrams = [' '.join(gram) for gram in ngrams(tokens, 2)]  # Generate bigrams\n",
        "    return bigrams\n",
        "\n",
        "# Apply preprocessing and create vocabulary\n",
        "train_data['tokens'] = train_data['statement'].apply(preprocess_text_with_bigrams)\n",
        "all_bigrams = [bigram for bigrams in train_data['tokens'] for bigram in bigrams]\n",
        "vocab = list(set(all_bigrams))\n",
        "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "n_observations = len(vocab)\n",
        "\n",
        "# Encode tokens and labels\n",
        "def encode_tokens(tokens):\n",
        "    return [word_to_index.get(token, -1) for token in tokens if token in word_to_index]\n",
        "\n",
        "encoded_train_sequences = [encode_tokens(tokens) for tokens in train_data['tokens']]\n",
        "encoded_state_sequences = [[state_mapping[label]] for label in train_data['label']]\n"
      ],
      "metadata": {
        "id": "2RZGQCIGODsw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the Regularized HMM\n",
        "hmm_model = RegularizedHMM(n_states=n_states, n_observations=n_observations, regularization=1e-3)\n",
        "hmm_model.train(encoded_train_sequences, encoded_state_sequences)\n",
        "print(\"Regularized HMM model training completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXi2hdXiOaif",
        "outputId": "8e44d818-8a9d-4480-b15b-1e5f005123c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regularized HMM model training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_sentence(sentence, hmm_model, word_to_index, state_mapping):\n",
        "    tokens = preprocess_text_with_bigrams(sentence)\n",
        "    encoded_tokens = encode_tokens(tokens)\n",
        "    if not encoded_tokens:\n",
        "        return \"Unknown\"  # If no tokens are recognized\n",
        "\n",
        "    # Use the Viterbi algorithm to find the best state path\n",
        "    best_path = hmm_model.viterbi(encoded_tokens)\n",
        "    return list(state_mapping.keys())[best_path[0]]\n",
        "\n",
        "# Test with a sample sentence\n",
        "sample_sentence = \"This is an entirely true statement.\"\n",
        "classification = classify_sentence(sample_sentence, hmm_model, word_to_index, state_mapping)\n",
        "print(f\"The sentence classification is: {classification}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Adm5KuYYOd5P",
        "outputId": "b98cad89-697e-4a23-86af-6617c6cc0505"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentence classification is: Unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the test set with bigrams\n",
        "test_data['tokens'] = test_data['statement'].apply(preprocess_text_with_bigrams)\n",
        "encoded_test_sequences = [encode_tokens(tokens) for tokens in test_data['tokens']]\n",
        "true_labels = [state_mapping[label] for label in test_data['label']]\n",
        "\n",
        "# Predict labels using the model\n",
        "predicted_labels = [hmm_model.viterbi(seq)[0] for seq in encoded_test_sequences]\n",
        "\n",
        "# Evaluation metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjLtrkQlOhok",
        "outputId": "1e17ea66-7c6d-4bb1-e966-70a397ca154f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.35\n",
            "Precision: 0.31\n",
            "Recall: 0.35\n",
            "F1 Score: 0.30\n"
          ]
        }
      ]
    }
  ]
}