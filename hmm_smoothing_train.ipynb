{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqScUonGTeQdvD4UwCOIE8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NITHIN-KANDI/PR_PROJECT/blob/main/hmm_smoothing_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "\n",
        "# Ensure NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define the HMM model components with Laplace smoothing\n",
        "class HMM:\n",
        "    def __init__(self, n_states, n_observations):\n",
        "        self.n_states = n_states  # Number of hidden states\n",
        "        self.n_observations = n_observations  # Number of possible observations\n",
        "\n",
        "        # Initialize the transition probabilities (states to states)\n",
        "        self.transition_probs = np.full((n_states, n_states), 1.0 / n_states)\n",
        "\n",
        "        # Initialize the emission probabilities (states to observations)\n",
        "        self.emission_probs = np.full((n_states, n_observations), 1.0 / n_observations)\n",
        "\n",
        "        # Initialize start probabilities\n",
        "        self.start_probs = np.full(n_states, 1.0 / n_states)\n",
        "\n",
        "    def train(self, sequences, state_sequences, alpha=1.0):\n",
        "        # Calculate start probabilities with smoothing\n",
        "        for state_seq in state_sequences:\n",
        "            self.start_probs[state_seq[0]] += 1\n",
        "        self.start_probs += alpha  # Laplace smoothing\n",
        "        self.start_probs /= self.start_probs.sum()\n",
        "\n",
        "        # Calculate transition probabilities with smoothing\n",
        "        for state_seq in state_sequences:\n",
        "            for i in range(len(state_seq) - 1):\n",
        "                self.transition_probs[state_seq[i], state_seq[i + 1]] += 1\n",
        "        self.transition_probs += alpha  # Laplace smoothing\n",
        "        self.transition_probs /= self.transition_probs.sum(axis=1, keepdims=True)\n",
        "\n",
        "        # Calculate emission probabilities with smoothing\n",
        "        for seq, state_seq in zip(sequences, state_sequences):\n",
        "            for obs, state in zip(seq, state_seq):\n",
        "                self.emission_probs[state, obs] += 1\n",
        "        self.emission_probs += alpha  # Laplace smoothing\n",
        "        self.emission_probs /= self.emission_probs.sum(axis=1, keepdims=True)\n",
        "\n",
        "    def viterbi(self, sequence):\n",
        "        # Initialize matrices\n",
        "        T = len(sequence)\n",
        "        viterbi_probs = np.zeros((self.n_states, T))\n",
        "        backpointer = np.zeros((self.n_states, T), dtype=int)\n",
        "\n",
        "        # Initialize the first column\n",
        "        for s in range(self.n_states):\n",
        "            viterbi_probs[s, 0] = self.start_probs[s] * self.emission_probs[s, sequence[0]]\n",
        "            backpointer[s, 0] = 0\n",
        "\n",
        "        # Fill the Viterbi matrix\n",
        "        for t in range(1, T):\n",
        "            for s in range(self.n_states):\n",
        "                probabilities = viterbi_probs[:, t - 1] * self.transition_probs[:, s] * self.emission_probs[s, sequence[t]]\n",
        "                viterbi_probs[s, t] = np.max(probabilities)\n",
        "                backpointer[s, t] = np.argmax(probabilities)\n",
        "\n",
        "        # Backtrace to get the most probable state sequence\n",
        "        best_path = np.zeros(T, dtype=int)\n",
        "        best_path[-1] = np.argmax(viterbi_probs[:, T - 1])\n",
        "        for t in range(T - 2, -1, -1):\n",
        "            best_path[t] = backpointer[best_path[t + 1], t + 1]\n",
        "\n",
        "        return best_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1cWe67CFzUy",
        "outputId": "c929d83a-1b4a-4bff-8332-43ad8680e12f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load filtered datasets\n",
        "train_data = pd.read_csv('/content/filtered_test_data.csv')\n",
        "test_data = pd.read_csv('/content/filtered_test_data.csv')\n",
        "\n",
        "# Define state mapping and vocabulary\n",
        "state_mapping = {'truth': 0, 'partial truth': 1, 'partial lie': 2, 'lie': 3}\n",
        "n_states = len(state_mapping)\n",
        "\n",
        "# Initialize stopwords and stemmer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize, remove stopwords, and stem\n",
        "    tokens = [stemmer.stem(word) for word in word_tokenize(text.lower()) if word.isalnum() and word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing and create vocabulary\n",
        "train_data['tokens'] = train_data['statement'].apply(preprocess_text)\n",
        "all_tokens = [token for tokens in train_data['tokens'] for token in tokens]\n",
        "vocab = list(set(all_tokens))\n",
        "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "n_observations = len(vocab)\n",
        "\n",
        "# Encode tokens and labels\n",
        "def encode_tokens(tokens):\n",
        "    return [word_to_index.get(token, -1) for token in tokens if token in word_to_index]\n",
        "\n",
        "encoded_train_sequences = [encode_tokens(tokens) for tokens in train_data['tokens']]\n",
        "encoded_state_sequences = [[state_mapping[label]] for label in train_data['label']]\n"
      ],
      "metadata": {
        "id": "L0ZwvP_RF0wh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the HMM with smoothing\n",
        "hmm_model = HMM(n_states=n_states, n_observations=n_observations)\n",
        "hmm_model.train(encoded_train_sequences, encoded_state_sequences, alpha=1.0)  # Alpha can be adjusted for smoothing\n",
        "print(\"HMM model training completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pncjXc4F_DP",
        "outputId": "1402a887-ae85-43ad-b282-d73c646389d9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HMM model training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_sentence(sentence, hmm_model, word_to_index, state_mapping):\n",
        "    # Tokenize and encode the sentence\n",
        "    tokens = preprocess_text(sentence)\n",
        "    encoded_tokens = [word_to_index.get(token, -1) for token in tokens if token in word_to_index]\n",
        "    if not encoded_tokens:\n",
        "        return \"Unknown\"  # If no tokens are recognized\n",
        "\n",
        "    # Use the Viterbi algorithm to find the best state path\n",
        "    best_path = hmm_model.viterbi(encoded_tokens)\n",
        "    return list(state_mapping.keys())[best_path[0]]\n",
        "\n",
        "# Test with a sample sentence\n",
        "sample_sentence = \"This is an entirely true statement.\"\n",
        "classification = classify_sentence(sample_sentence, hmm_model, word_to_index, state_mapping)\n",
        "print(f\"The sentence classification is: {classification}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9NXirIsGDHV",
        "outputId": "9c1d111d-23dc-4d0d-b474-6a36be89f3e7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentence classification is: truth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the test set\n",
        "test_data['tokens'] = test_data['statement'].apply(preprocess_text)\n",
        "encoded_test_sequences = [encode_tokens(tokens) for tokens in test_data['tokens']]\n",
        "true_labels = [state_mapping[label] for label in test_data['label']]\n",
        "# Predict labels using the model, with error handling for empty sequences\n",
        "most_common_label = max(state_mapping, key=list(state_mapping.values()).count)  # Default label if needed\n",
        "predicted_labels = []\n",
        "\n",
        "for seq in encoded_test_sequences:\n",
        "    if len(seq) > 0:  # Check if the sequence is non-empty\n",
        "        predicted_state = hmm_model.viterbi(seq)[0]\n",
        "    else:\n",
        "        predicted_state = state_mapping[most_common_label]  # Default label for empty sequences\n",
        "    predicted_labels.append(predicted_state)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Evaluation metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEh56g_8GGmZ",
        "outputId": "09237ef8-4320-4e43-a3cc-9640b6e16511"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.52\n",
            "Precision: 0.62\n",
            "Recall: 0.52\n",
            "F1 Score: 0.47\n"
          ]
        }
      ]
    }
  ]
}