{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXfjLGvbllje9IJhXEP7Ra",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NITHIN-KANDI/PR_PROJECT/blob/main/hmm_gussianemmission_regularzation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtK6PlEIMfDA",
        "outputId": "40347193-fc45-48be-c15d-b0e43473c4a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk import ngrams\n",
        "import nltk\n",
        "\n",
        "# Ensure NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class EnhancedHMM:\n",
        "    def __init__(self, n_states, n_observations, n_mixtures=2, regularization=1e-6):\n",
        "        self.n_states = n_states\n",
        "        self.n_observations = n_observations\n",
        "        self.n_mixtures = n_mixtures\n",
        "        self.regularization = regularization\n",
        "\n",
        "        # Initialize transition probabilities (add regularization)\n",
        "        self.transition_probs = np.full((n_states, n_states), 1.0 / n_states) + regularization\n",
        "\n",
        "        # Initialize emission probabilities as Gaussian Mixtures\n",
        "        self.gmms = [GaussianMixture(n_components=n_mixtures, covariance_type='full') for _ in range(n_states)]\n",
        "\n",
        "        # Initialize start probabilities with regularization\n",
        "        self.start_probs = np.full(n_states, 1.0 / n_states) + regularization\n",
        "\n",
        "    def train(self, sequences, state_sequences):\n",
        "        # Update start probabilities with regularization\n",
        "        for state_seq in state_sequences:\n",
        "            self.start_probs[state_seq[0]] += 1\n",
        "        self.start_probs = (self.start_probs + self.regularization) / (self.start_probs.sum())\n",
        "\n",
        "        # Update transition probabilities with regularization\n",
        "        for state_seq in state_sequences:\n",
        "            for i in range(len(state_seq) - 1):\n",
        "                self.transition_probs[state_seq[i], state_seq[i + 1]] += 1\n",
        "        self.transition_probs = (self.transition_probs + self.regularization)\n",
        "        self.transition_probs /= self.transition_probs.sum(axis=1, keepdims=True)\n",
        "\n",
        "        # Train Gaussian Mixture Models for emission probabilities\n",
        "        for state in range(self.n_states):\n",
        "            state_observations = [obs for seq, state_seq in zip(sequences, state_sequences)\n",
        "                                  for obs, s in zip(seq, state_seq) if s == state]\n",
        "            if state_observations:\n",
        "                self.gmms[state].fit(state_observations)\n",
        "\n",
        "    def viterbi(self, sequence):\n",
        "        T = len(sequence)\n",
        "        if T == 0:\n",
        "            return [0]  # Return default state if sequence is empty\n",
        "\n",
        "        # Initialize Viterbi matrices\n",
        "        viterbi_probs = np.zeros((self.n_states, T))\n",
        "        backpointer = np.zeros((self.n_states, T), dtype=int)\n",
        "\n",
        "        # Initialize the first column with Gaussian mixture probabilities\n",
        "        for s in range(self.n_states):\n",
        "            viterbi_probs[s, 0] = self.start_probs[s] * np.exp(self.gmms[s].score([sequence[0]]))\n",
        "\n",
        "        # Fill in the Viterbi matrix\n",
        "        for t in range(1, T):\n",
        "            for s in range(self.n_states):\n",
        "                emission_prob = np.exp(self.gmms[s].score([sequence[t]]))\n",
        "                transition_emission_probs = viterbi_probs[:, t - 1] * self.transition_probs[:, s] * emission_prob\n",
        "                viterbi_probs[s, t] = np.max(transition_emission_probs)\n",
        "                backpointer[s, t] = np.argmax(transition_emission_probs)\n",
        "\n",
        "        # Backtrace to get the most probable state sequence\n",
        "        best_path = np.zeros(T, dtype=int)\n",
        "        best_path[-1] = np.argmax(viterbi_probs[:, T - 1])\n",
        "        for t in range(T - 2, -1, -1):\n",
        "            best_path[t] = backpointer[best_path[t + 1], t + 1]\n",
        "\n",
        "        return best_path\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load filtered datasets\n",
        "train_data = pd.read_csv('/content/filtered_train_data.csv')\n",
        "test_data = pd.read_csv('/content/filtered_test_data.csv')\n",
        "\n",
        "# Define state mapping and vocabulary\n",
        "state_mapping = {'truth': 0, 'partial truth': 1, 'partial lie': 2, 'lie': 3}\n",
        "n_states = len(state_mapping)\n",
        "\n",
        "# Initialize stopwords and stemmer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text_with_bigrams(text):\n",
        "    # Tokenize, remove stopwords, and stem\n",
        "    tokens = [stemmer.stem(word) for word in word_tokenize(text.lower()) if word.isalnum() and word not in stop_words]\n",
        "    bigrams = [' '.join(gram) for gram in ngrams(tokens, 2)]  # Generate bigrams\n",
        "    return bigrams\n",
        "\n",
        "# Apply preprocessing and create vocabulary\n",
        "train_data['tokens'] = train_data['statement'].apply(preprocess_text_with_bigrams)\n",
        "all_bigrams = [bigram for bigrams in train_data['tokens'] for bigram in bigrams]\n",
        "vocab = list(set(all_bigrams))\n",
        "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "n_observations = len(vocab)\n",
        "\n",
        "# Encode tokens and labels\n",
        "def encode_tokens(tokens):\n",
        "    return [[word_to_index.get(token, -1)] for token in tokens if token in word_to_index]\n",
        "\n",
        "encoded_train_sequences = [encode_tokens(tokens) for tokens in train_data['tokens']]\n",
        "encoded_state_sequences = [[state_mapping[label]] for label in train_data['label']]\n"
      ],
      "metadata": {
        "id": "2pyB6Z3EMxo1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the enhanced HMM\n",
        "hmm_model = EnhancedHMM(n_states=n_states, n_observations=n_observations, n_mixtures=2, regularization=1e-6)\n",
        "hmm_model.train(encoded_train_sequences, encoded_state_sequences)\n",
        "print(\"Enhanced HMM model training completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfNswsQYNFYF",
        "outputId": "bc332df6-3c0e-4cac-dd94-0b803480966e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced HMM model training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_sentence(sentence, hmm_model, word_to_index, state_mapping):\n",
        "    tokens = preprocess_text_with_bigrams(sentence)\n",
        "    encoded_tokens = encode_tokens(tokens)\n",
        "    if not encoded_tokens:\n",
        "        return \"Unknown\"  # If no tokens are recognized\n",
        "\n",
        "    # Use the Viterbi algorithm to find the best state path\n",
        "    best_path = hmm_model.viterbi(encoded_tokens)\n",
        "    return list(state_mapping.keys())[best_path[0]]\n",
        "\n",
        "# Test with a sample sentence\n",
        "sample_sentence = \"This is an entirely true statement.\"\n",
        "classification = classify_sentence(sample_sentence, hmm_model, word_to_index, state_mapping)\n",
        "print(f\"The sentence classification is: {classification}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "153DI6WlNI6v",
        "outputId": "2e152425-a815-4d87-ef03-9066fd2a39f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentence classification is: Unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the test set with bigrams\n",
        "test_data['tokens'] = test_data['statement'].apply(preprocess_text_with_bigrams)\n",
        "encoded_test_sequences = [encode_tokens(tokens) for tokens in test_data['tokens']]\n",
        "true_labels = [state_mapping[label] for label in test_data['label']]\n",
        "\n",
        "# Predict labels using the model\n",
        "predicted_labels = [hmm_model.viterbi(seq)[0] for seq in encoded_test_sequences]\n",
        "\n",
        "# Evaluation metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErsfvhhQNMFq",
        "outputId": "6b6a53c4-feb9-4fa3-ca2b-68c1e6e67c25"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.36\n",
            "Precision: 0.26\n",
            "Recall: 0.36\n",
            "F1 Score: 0.29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}